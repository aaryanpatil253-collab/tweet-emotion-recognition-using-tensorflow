# -*- coding: utf-8 -*-
"""tweet emotion recognition using tensorflow

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1R0GLrO7pEE1PPrKyG55VHp8DYVZfIOis
"""

! pip install nlp

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import nlp
import random

def show_history(h):
  epochs_trained = len(h.history['loss'])
  plt.figure(figsize=(16,6))

  plt.subplot(1,2,1)
  plt.plot(range(0, epochs_trained), h.history.get('accuracy'), label='Training')
  plt.plot(range(0, epochs_trained), h.history.get('val_accuracy'), label='Validation')
  plt.ylim([0.,1.])
  plt.xlabel('Epochs')
  plt.ylabel('Accuracy')
  plt.legend()

  plt.subplot(1,2,2)
  plt.plot(range(0, epochs_trained), h.history.get('loss'), label='Training')
  plt.plot(range(0, epochs_trained), h.history.get('val_loss'), label='Validation')
  plt.xlabel('Epochs')
  plt.ylabel('Accuracy')
  plt.legend()
  plt.show()

def show_confusion_matrix(y_true, y_pred, classes):
  from sklearn.metrics import confusion_matrix

  cm = confusion_matrix(y_true,y_pred, normalize='true')

  plt.figure(figsize=(8,8))
  sp = plt.subplot(1,1,1)
  ctx = sp.matshow(cm)
  plt.xticks(list(range(0,6)), labels=classes)
  plt.yticks(list(range(0,6)), labels=classes)
  plt.colorbar(ctx)
  plt.show()

print('Using TensorFlow vsersion', tf.__version__)

train = "/content/drive/MyDrive/tweet/train.txt"
val = "/content/drive/MyDrive/tweet/val.txt"
test  ="/content/drive/MyDrive/tweet/test.txt"

import sys

def get_tweet(data):
    tweets = []
    labels = []
    for x in data:
        try:
            tweet, label = x.split(';')
            tweets.append(tweet)
            labels.append(label)
        except IndexError:
            print(f"Skipping malformed data entry: {x}", file=sys.stderr)
    return tweets, labels

loaded_labels = ['anger', 'surprise', 'love', 'sadness', 'joy', 'fear']

# Assuming train_dataset_dict is already defined and loaded
tweets, labels = get_tweet(train_dataset_dict['text'])
display((tweets[0], labels[0]))

from tensorflow.keras.preprocessing.text import Tokenizer

tokenizer = Tokenizer(num_words=10000, oov_token="<UNK>")
tokenizer.fit_on_texts(tweets)

tokenizer.fit_on_sequences([tweets[0]])

tweets[0]

lenghts = [len(t.split(' ')) for t in tweets]
plt.hist(lenghts, bins = len(set(lenghts)))
plt.show()

maxlen = 50
from tensorflow.keras.preprocessing.sequence import pad_sequences

from os import truncate
from typing import Sequence
def get_sequence(tokenizer, tweets):
  sequences = tokenizer.texts_to_sequences(tweets)
  padded = pad_sequences(sequences,
                         maxlen = maxlen,
                         truncating = 'post',
                         padding = 'post')
  return padded

padded_train_seq = get_sequence(tokenizer, tweets)

padded_train_seq[0]

classes = set(labels)
print(classes)

plt.hist(labels, bins = 11)
plt.show()

class_to_index = dict((c, i) for i, c in enumerate(classes))
index_to_class = dict((v, k) for k, v in class_to_index.items())

class_to_index

index_to_class

def names_to_ids(labels, class_to_index):
  return np.array([class_to_index[label] for label in labels])

model = tf.keras.models.Sequential([
    tf.keras.layers.Embedding(10000, 16, input_length=maxlen),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(20, return_sequences=True)),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(20)),
    tf.keras.layers.Dense(6, activation='softmax')
])

model.compile(
    loss = 'sparse_categorical_crossentropy',
    optimizer = 'adam',
    metrics=['accuracy']
)

model.summary()

with open(val, 'r') as f:
    val_data = f.readlines()

val_tweets, val_labels = get_tweet(val_data)
val_seq = get_sequence(tokenizer, val_tweets)
val_labels = names_to_ids(val_labels, class_to_index)

val_tweets[0], val_labels[0]

h = model.fit(
    padded_train_seq, train_labels,
    validation_data = (val_seq, val_labels),
    epochs = 20,
    callbacks=[
        tf.keras.callbacks.EarlyStopping(monitor = 'val_accuracy', patience=2)
    ]
)

show_history(h)

with open(test, 'r') as f:
    test_data = f.readlines()

test_tweets, test_labels = get_tweet(test_data)
test_seq = get_sequence(tokenizer, test_tweets)
test_labels = names_to_ids(test_labels, class_to_index)

_ = model.evaluate(test_seq, test_labels)

i = random.randint(0, len(test_labels) - 1)

print('Sentence:', test_tweets[i])
print('Emotion:', index_to_class[test_labels[i]])

p = model.predict(np.expand_dims(test_seq[i], axis = 0))[0]
pred_class = index_to_class[np.argmax(p).astype('uint8')]

print('Predicted Emotion:', pred_class)

preds= model.predict(test_seq)
classes_x=np.argmax(preds,axis=1)

show_confusion_matrix(test_labels, classes_x, list(classes))